# language (日志啥的语言类型)
language: zh
# task type(ner,cls,generate)
type: GEN
model: GPT
struct: GPT-ConditionalGeneration-QA
# required params (常规必备参数)
base: 
  task_name: gen_test_QA_GPT
  data_dir: /data/anon/anon-nlp-toolkit/datasets/gen_QA/medical_QA
  model_name_or_path: /data/anon/pretrain_model/CDial-GPT_LCCC-base
  output_dir: /data/anon/output_dir
  cache_dir: /data/anon/cache_dir
    # 在此作为QA-答案的最大长度
  train_max_seq_len: 512
  eval_max_seq_len: 512
  # 在此作为QA-问题的最大长度
  train_max_utterance_len: 256
  eval_max_utterance_len: 256
  do_train: true
  do_eval: true
  do_predict: false
  evaluate_during_training: true
  do_lower_case: true
  per_gpu_train_batch_size: 8
  per_gpu_eval_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 0.00005
  weight_decay: 0.01
  adam_epsilon: 0.00000001
  max_grad_norm: 1.0
  num_train_epochs: 2
  max_steps: -1
  warmup_proportion: 0.1
  logging_steps: 132500
  save_steps: 265000
  eval_all_checkpoints: true
  predict_checkpoints: 40
  no_cuda: false
  overwrite_output_dir: true
  overwrite_cache: false
  seed: 42
  fp16: false
  fp16_opt_level: O1
  local_rank: -1
  server_ip:
  server_port:



# (adversarial training params) 对抗训练参数
adversarial:
  do_adv: true
  adv_epsilon: 1.0
  adv_name: word_embeddings


gen_special:
  # 是否要在保存每一个ckpt之前进行text评估（bleu,rouge,distinct,gleu）
  text_eval_before_save: true
  # 如果存在多个问题，该如何选择 0-用最长的，1-都用
  muti_question_strategy: 0

  



  




  
